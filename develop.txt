1. Save the goal and skill from policy_step to replay buffer, imagine rollout
2. Check the distributions of Manager, Worker, 
    Done
3. Check the distributions GoalEncoder and GoalDecoder and corresponding fitting
    Done
4. Check Device, backpropagation and optimization related code

5. context_latent.shape, model_context_action.shape, prior_flattened_sample.shape, last_dist_feat.shape
    (torch.Size([1, 1, 1024]), torch.Size([1, 1]), torch.Size([1, 1, 1024]), torch.Size([1, 1, 512]))

# buffer just holds the data realted to environment;
when that data is loaded. the WM.imagine_data() function converts the environmental data to latent sample, etc....
So the agent related data can also be dealt like that. The agent can store goal and skill. 
Moreoever the stoing is only necesaay withon one contenxt length L

TODOs:
1. Check the losses why are some in negative do they make sense?
2. Inspect sanity if all the things are workign by inspecting them like if the goals are working if the skill is working
3. Track diagonistics data like if the weights are changing or not stuff like that
4. Then think logically if something is not working!!

#Multi Envs:
1. Check the models action is a list for all envs. agent.sample_as_env_action()
    Yes. Existing code autoamtically handles that as an extra dimension
2. Replay buffere randomizing indexes but it the same across envs. That means similar timeframes of the 4 envs are chooses although the tiem frames are randomized.
when 0->16 chosen, then its 0-16 for all the envs. Next 5-21 for all the 4 envs.
Yes. In RL training (especially model-based), once you’ve collected transitions, you typically don’t care which env they came from. What matters is that:
They’re valid trajectories of length T.
They're not crossing episode boundaries (you should ensure this).
The model gets enough diverse samples.