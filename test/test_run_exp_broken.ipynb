{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import ale_py\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import colorama\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import os\n",
    "# import wandb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1 cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/data/I6347325/work_space/STORM\"))# Dynamically reload the modules to reflect any changes\n",
    "\n",
    "import utils\n",
    "import sub_models.replay_buffer\n",
    "import env_wrapper\n",
    "# import agents\n",
    "import sub_models.director_agents\n",
    "import sub_models.functions_losses\n",
    "import sub_models.world_models\n",
    "import sub_models.constants\n",
    "import train\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(sub_models.replay_buffer)\n",
    "importlib.reload(env_wrapper)\n",
    "importlib.reload(sub_models.director_agents)\n",
    "importlib.reload(sub_models.functions_losses)\n",
    "importlib.reload(sub_models.world_models)\n",
    "importlib.reload(sub_models.constants)\n",
    "importlib.reload(train)\n",
    "\n",
    "from utils import seed_np_torch, Logger, load_config\n",
    "from sub_models.replay_buffer import ReplayBuffer\n",
    "from train import (\n",
    "    build_single_env,\n",
    "    build_vec_env,\n",
    "    build_world_model,\n",
    "    build_agent,\n",
    "    train_world_model,\n",
    "    world_model_imagine_data,\n",
    "    joint_train_world_model_agent,\n",
    ")\n",
    "from sub_models.constants import DEVICE\n",
    "print(DEVICE, DEVICE.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mArguments:\u001b[0m\n",
      "\u001b[32m-----------------\u001b[0m\n",
      "\u001b[32mexp_name: \u001b[0mTEM-Transformer_1\n",
      "\u001b[32mseed: \u001b[0m1\n",
      "\u001b[32mconfig_path: \u001b[0m../config_files/STORM.yaml\n",
      "\u001b[32mtrajectory_path: \u001b[0mD_TRAJ/MsPacman.pkl\n",
      "\u001b[32menv_name: \u001b[0mALE/MsPacman-v5\n",
      "\u001b[32m-----------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "class RunParams:\n",
    "    def __init__(self, env_name=\"MsPacman\", exp_name = \"TEM-Transformer\"):\n",
    "        self._env_name = env_name\n",
    "        self.exp_name = exp_name\n",
    "        self.seed = 1\n",
    "        self.config_path = \"../config_files/STORM.yaml\"\n",
    "        self.trajectory_path = f\"D_TRAJ/{self._env_name}.pkl\"\n",
    "        self.env_name = f\"ALE/{self._env_name}-v5\"\n",
    "\n",
    "        self.conf = load_config(self.config_path)\n",
    "        self.print_args()\n",
    "    def print_args(self):\n",
    "        print(colorama.Fore.GREEN + \"Arguments:\" + colorama.Style.RESET_ALL)\n",
    "        print(colorama.Fore.GREEN + \"-----------------\" + colorama.Style.RESET_ALL)\n",
    "        print(colorama.Fore.GREEN + \"exp_name: \" + colorama.Style.RESET_ALL + self.exp_name)\n",
    "        print(colorama.Fore.GREEN + \"seed: \" + colorama.Style.RESET_ALL + str(self.seed))\n",
    "        print(colorama.Fore.GREEN + \"config_path: \" + colorama.Style.RESET_ALL + self.config_path)\n",
    "        print(colorama.Fore.GREEN + \"trajectory_path: \" + colorama.Style.RESET_ALL + self.trajectory_path)\n",
    "        print(colorama.Fore.GREEN + \"env_name: \" + colorama.Style.RESET_ALL + self.env_name)\n",
    "        print(colorama.Fore.GREEN + \"-----------------\" + colorama.Style.RESET_ALL)\n",
    "    \n",
    "    # def get_configs(self):\n",
    "        \n",
    "    #     config_dict = {\n",
    "    #         \"env_ImageSize\": self.conf[\"BasicSettings\"][\"ImageSize\"],\n",
    "    #         \"env_ReplayBufferOnGPU\": self.conf[\"BasicSettings\"][\"ReplayBufferOnGPU\"],\n",
    "    #         \"WM_InChannels\": self.conf[\"Models\"][\"WorldModel\"][\"InChannels\"],\n",
    "    #         \"WM_TransformerMaxLength\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerMaxLength\"],\n",
    "    #         \"WM_TransformerHiddenDim\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerHiddenDim\"],\n",
    "    #         \"WM_TransformerNumLayers\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerNumLayers\"],\n",
    "    #         \"WM_TransformerNumHeads\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerNumHeads\"],\n",
    "    #         \"Agent_NumLayers\": self.conf[\"Models\"][\"Agent\"][\"NumLayers\"],\n",
    "    #         \"Agent_HiddenDim\": self.conf[\"Models\"][\"Agent\"][\"HiddenDim\"],\n",
    "    #         \"Agent_Gamma\": self.conf[\"Models\"][\"Agent\"][\"Gamma\"],\n",
    "    #         \"Agent_Lambda\": self.conf[\"Models\"][\"Agent\"][\"Lambda\"],\n",
    "    #         \"Agent_EntropyCoef\": self.conf[\"Models\"][\"Agent\"][\"EntropyCoef\"],\n",
    "    #         \"Train_MaxSteps\": self.conf[\"JointTrainAgent\"][\"SampleMaxSteps\"],\n",
    "    #         \"Train_BufferMaxLength\": self.conf[\"JointTrainAgent\"][\"BufferMaxLength\"],\n",
    "    #         \"Train_BufferWarmUp\": self.conf[\"JointTrainAgent\"][\"BufferWarmUp\"],\n",
    "    #         \"Train_NumEnvs\": self.conf[\"JointTrainAgent\"][\"NumEnvs\"],\n",
    "    #         \"Train_BatchSize\": self.conf[\"JointTrainAgent\"][\"BatchSize\"],\n",
    "    #         \"Train_DemonstrationBatchSize\": self.conf[\"JointTrainAgent\"][\"DemonstrationBatchSize\"],\n",
    "    #         \"Train_BatchLength\": self.conf[\"JointTrainAgent\"][\"BatchLength\"],\n",
    "    #         \"Train_ImagineBatchSize\": self.conf[\"JointTrainAgent\"][\"ImagineBatchSize\"],\n",
    "    #         \"Train_ImagineDemonstrationBatchSize\": self.conf[\"JointTrainAgent\"][\"ImagineDemonstrationBatchSize\"],\n",
    "    #         \"Train_ImagineContextLength\": self.conf[\"JointTrainAgent\"][\"ImagineContextLength\"],\n",
    "    #         \"Train_ImagineBatchLength\": self.conf[\"JointTrainAgent\"][\"ImagineBatchLength\"],\n",
    "    #         \"Train_TrainDynamicsEverySteps\": self.conf[\"JointTrainAgent\"][\"TrainDynamicsEverySteps\"],\n",
    "    #         \"Train_TrainAgentEverySteps\": self.conf[\"JointTrainAgent\"][\"TrainAgentEverySteps\"],\n",
    "    #         \"Train_SaveEverySteps\": self.conf[\"JointTrainAgent\"][\"SaveEverySteps\"],\n",
    "    #         \"Train_UseDemonstration\": self.conf[\"JointTrainAgent\"][\"UseDemonstration\"],\n",
    "    #     }\n",
    "    #     return config_dict\n",
    "\n",
    "run_params = RunParams(env_name=\"MsPacman\", exp_name = \"TEM-Transformer_1\")\n",
    "# set seed\n",
    "seed_np_torch(seed=run_params.seed)\n",
    "# tensorboard writer\n",
    "logger = Logger(path=f\"runs/{run_params.exp_name}\")\n",
    "# copy config file\n",
    "# shutil.copy(run_params.config_path, f\"runs/{run_params.exp_name}/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World model transformer: StochasticTransformerKVCache\n",
      "World model parameters: 16508547\n",
      "Agent parameters: 5367561\n"
     ]
    }
   ],
   "source": [
    "# Setuop env, models, replay buffer\n",
    "# getting action_dim with dummy env\n",
    "dummy_env = build_single_env(\n",
    "    run_params.env_name, run_params.conf.BasicSettings.ImageSize, seed=0\n",
    ")\n",
    "action_dim = dummy_env.action_space.n\n",
    "\n",
    "# build world model and agent\n",
    "world_model = build_world_model(run_params.conf, action_dim)\n",
    "agent = build_agent(run_params.conf, action_dim)\n",
    "print(f\"World model transformer: {world_model.storm_transformer.__class__.__name__}\")\n",
    "# Log the number of parameters for both models\n",
    "world_model_params = sum(p.numel() for p in world_model.parameters() if p.requires_grad)\n",
    "agent_params = sum(p.numel() for p in agent.parameters() if p.requires_grad)\n",
    "print(f\"World model parameters: {world_model_params}\")\n",
    "print(f\"Agent parameters: {agent_params}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown: joint_train_world_model_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/I6347325/miniconda3/envs/env_RL/lib/python3.13/site-packages/torchrl/data/replay_buffers/samplers.py:34: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
      "  warnings.warn(EXTENSION_WARNING)\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env: \u001b[33mALE/MsPacman-v5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## setup variable names for breakdown\n",
    "env_name=run_params.env_name\n",
    "num_envs=run_params.conf.JointTrainAgent.NumEnvs\n",
    "max_steps=run_params.conf.JointTrainAgent.SampleMaxSteps\n",
    "image_size=run_params.conf.BasicSettings.ImageSize\n",
    "train_dynamics_every_steps=run_params.conf.JointTrainAgent.TrainDynamicsEverySteps\n",
    "train_agent_every_steps=run_params.conf.JointTrainAgent.TrainAgentEverySteps\n",
    "batch_size=3 #FIXME: run_params.conf.JointTrainAgent.BatchSize\n",
    "demonstration_batch_size=(\n",
    "    run_params.conf.JointTrainAgent.DemonstrationBatchSize\n",
    "    if run_params.conf.JointTrainAgent.UseDemonstration\n",
    "    else 0\n",
    ")\n",
    "batch_length=16 #FIXME: run_params.conf.JointTrainAgent.BatchLength\n",
    "imagine_batch_size=run_params.conf.JointTrainAgent.ImagineBatchSize\n",
    "imagine_demonstration_batch_size=(\n",
    "    run_params.conf.JointTrainAgent.ImagineDemonstrationBatchSize\n",
    "    if run_params.conf.JointTrainAgent.UseDemonstration\n",
    "    else 0\n",
    ")\n",
    "imagine_context_length=run_params.conf.JointTrainAgent.ImagineContextLength\n",
    "imagine_batch_length=16 #FIXME: run_params.conf.JointTrainAgent.ImagineBatchLength\n",
    "save_every_steps=run_params.conf.JointTrainAgent.SaveEverySteps\n",
    "seed=run_params.seed\n",
    "args=run_params\n",
    "\n",
    "\n",
    "## Setup env\n",
    "vec_env = build_vec_env(env_name, image_size, num_envs=1, seed=seed)\n",
    "print(\n",
    "    \"Current env: \"\n",
    "    + colorama.Fore.YELLOW\n",
    "    + f\"{env_name}\"\n",
    "    + colorama.Style.RESET_ALL\n",
    ")\n",
    "\n",
    "# reset envs and variables\n",
    "sum_reward = np.zeros(num_envs)\n",
    "current_obs, current_info = vec_env.reset()\n",
    "context_obs = deque(maxlen=16)\n",
    "context_action = deque(maxlen=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build replay buffer\n",
    "replay_buffer = ReplayBuffer(\n",
    "    num_envs=run_params.conf.JointTrainAgent.NumEnvs,\n",
    "    obs_shape=(run_params.conf.BasicSettings.ImageSize, run_params.conf.BasicSettings.ImageSize, 3),\n",
    "    # agent_goal_shape=32 * 32,  # faltten sample dim\n",
    "    # agent_skill_shape=agent.skill_shape,\n",
    "    max_length=run_params.conf.JointTrainAgent.BufferMaxLength,\n",
    "    warmup_length=20,  #FIXME: run_params.conf.JointTrainAgent.BufferWarmUp,\n",
    "    store_on_gpu=run_params.conf.BasicSettings.ReplayBufferOnGPU,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from env part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer ready 21\n",
      "Replay buffer ready 22\n",
      "Replay buffer ready 23\n",
      "Replay buffer ready 24\n",
      "Replay buffer ready 25\n",
      "Replay buffer ready 26\n",
      "Replay buffer ready 27\n",
      "Replay buffer ready 28\n",
      "Replay buffer ready 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 363.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer ready 30\n",
      "Replay buffer ready 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for total_steps in tqdm(range(32)):\n",
    "    # sample part >>>\n",
    "    if replay_buffer.ready:  # ready only after warmpup\n",
    "        print(\"Replay buffer ready\", total_steps)\n",
    "        # WM and Agent are in eval mode\n",
    "        world_model.eval()\n",
    "        agent.eval()\n",
    "        with torch.no_grad():\n",
    "            if len(context_action) == 0:\n",
    "                # this is the case in the first step\n",
    "                action = vec_env.action_space.sample()\n",
    "            else:\n",
    "                context_latent = world_model.encode_obs(\n",
    "                    torch.cat(list(context_obs), dim=1)\n",
    "                )\n",
    "                model_context_action = np.stack(list(context_action), axis=1)\n",
    "                model_context_action = torch.Tensor(model_context_action).to(DEVICE)\n",
    "                prior_flattened_sample, last_dist_feat = (\n",
    "                    world_model.calc_last_dist_feat(\n",
    "                        context_latent, model_context_action\n",
    "                    )\n",
    "                )\n",
    "                latent = torch.cat([prior_flattened_sample, last_dist_feat], dim=-1)\n",
    "                # get the action, goal and skill from the agent\n",
    "                action = agent.sample_as_env_action(latent)\n",
    "        # [B, H, W, C] -> [B, 1, C, H, W] # B=1\n",
    "        context_obs.append(\n",
    "            torch.permute(\n",
    "                torch.tensor(current_obs, device=DEVICE), (0, 3, 1, 2)\n",
    "            ).unsqueeze(1)\n",
    "            / 255\n",
    "        )\n",
    "        context_action.append(action)\n",
    "    else:\n",
    "        # simply sample random action\n",
    "        action = vec_env.action_space.sample()\n",
    "\n",
    "    # Perform action in the env and observe the next state, reward, done, truncated\n",
    "    obs, reward, done, truncated, info = vec_env.step(action)\n",
    "\n",
    "    # Append the transition to the replay buffer\n",
    "    replay_buffer.append(\n",
    "        current_obs, action, reward, np.logical_or(done, info[\"life_loss\"])\n",
    "        )\n",
    "\n",
    "    done_flag = np.logical_or(done, truncated)\n",
    "    if done_flag.any():  # end of episode\n",
    "        for i in range(num_envs):\n",
    "            if done_flag[i]:\n",
    "                sum_reward[i] = 0\n",
    "\n",
    "    # Update current_obs, current_info and sum_reward\n",
    "    sum_reward += reward\n",
    "    current_obs = obs\n",
    "    current_info = info\n",
    "    # <<< sample part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 64, 64, 3), (1,), (1,), (1,), (1,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape, action.shape, reward.shape, done.shape, truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train world model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train world model part >>>\n",
    "wm_metrics = train_world_model(\n",
    "    replay_buffer=replay_buffer,\n",
    "    world_model=world_model,\n",
    "    batch_size=batch_size,\n",
    "    demonstration_batch_size=demonstration_batch_size,\n",
    "    batch_length=batch_length,\n",
    "    # logger=logger,\n",
    ")\n",
    "##<<< Train world model part\n",
    "## Breakdown of the above code\n",
    "# Sample from replay buffer\n",
    "# buffer_sample = replay_buffer.sample(\n",
    "#     batch_size, demonstration_batch_size, batch_length\n",
    "# )\n",
    "# for key, value in buffer_sample.items():\n",
    "#     print(f\"{key}, Value shape: {value.shape}\")\n",
    "# obs, Value shape: torch.Size([3, 16, 3, 64, 64])\n",
    "# action, Value shape: torch.Size([3, 16])\n",
    "# reward, Value shape: torch.Size([3, 16])\n",
    "# termination, Value shape: torch.Size([3, 16])\n",
    "# goal, Value shape: torch.Size([3, 16])\n",
    "# skill, Value shape: torch.Size([3, 16])\n",
    "# print(f\"Shapes of obs: {obs.shape}, action: {action.shape}, reward: {reward.shape}, termination: {termination.shape}\")\n",
    "\n",
    "## Train world model with the sampled data\n",
    "# world_model.update(buffer_sample[\"obs\"], buffer_sample[\"action\"], buffer_sample[\"reward\"], buffer_sample[\"termination\"], logger=logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_imagine_buffer: 1024x16@torch.float32\n",
      "Shape of sample: torch.Size([1024, 16, 1024])\n",
      "Shape of hidden: torch.Size([1024, 16, 512])\n",
      "Shape of action: torch.Size([1024, 16])\n",
      "Shape of reward: torch.Size([1024, 16])\n",
      "Shape of termination: torch.Size([1024, 16])\n",
      "Shape of goal: torch.Size([1024, 16, 1024])\n",
      "Shape of skill: torch.Size([1024, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Train agent part >>>\n",
    "# print(\"Training Agent...\")\n",
    "log_video = False\n",
    "imagined_rollout = world_model_imagine_data(\n",
    "    replay_buffer=replay_buffer,\n",
    "    world_model=world_model,\n",
    "    agent=agent,\n",
    "    imagine_batch_size=imagine_batch_size,\n",
    "    imagine_demonstration_batch_size=imagine_demonstration_batch_size,\n",
    "    imagine_context_length=imagine_context_length,\n",
    "    imagine_batch_length=imagine_batch_length,\n",
    "    log_video=log_video,\n",
    "    # logger=logger,\n",
    ")\n",
    "for k, v  in imagined_rollout.items():\n",
    "    print(f\"Shape of {k}: {v.shape}\")\n",
    "\n",
    "## breakdown : world_model_imagine_data\n",
    "# imagine_batch_size = 3\n",
    "# world_model.eval()\n",
    "# agent.eval()\n",
    "\n",
    "# buffer_sample = replay_buffer.sample(\n",
    "#     imagine_batch_size, imagine_demonstration_batch_size, imagine_context_length\n",
    "# )\n",
    "# print(f\"Buffer sample items:\")\n",
    "# for k, v  in buffer_sample.items():\n",
    "#     print(f\"Shape of {k}: {v.shape}\")\n",
    "\n",
    "# imagined_rollout = world_model.imagine_data(\n",
    "#     agent,\n",
    "#     buffer_sample,\n",
    "#     imagine_batch_size=imagine_batch_size + imagine_demonstration_batch_size,\n",
    "#     imagine_batch_length=imagine_batch_length,\n",
    "#     log_video=log_video,\n",
    "#     logger=logger,\n",
    "# )\n",
    "# print(f\"\\n\\nImagine rollout items:\")\n",
    "# for k, v  in imagined_rollout.items():\n",
    "#     print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_RL/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: clip() received an invalid combination of arguments - got (float, float, out=NoneType), but expected one of:\n * (Tensor min = None, Tensor max = None)\n * (Number min = None, Number max = None)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Update agent with imagined data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m metrics = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagined_rollout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# <<< Train agent part\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/I6347325/work_space/STORM/sub_models/director_agents.py:806\u001b[39m, in \u001b[36mDirectorAgent.update\u001b[39m\u001b[34m(self, imagine_rollout)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# --- Train VAE only (GoalEncoder + GoalDecoder) ---\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(\n\u001b[32m    804\u001b[39m     device_type=DEVICE.type, dtype=DTYPE_16, enabled=\u001b[38;5;28mself\u001b[39m.use_amp\n\u001b[32m    805\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     vae_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_goal_vae_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagine_rollout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m metrics.update(vae_metrics)\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# --- Train manager and worker independently ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/I6347325/work_space/STORM/sub_models/director_agents.py:676\u001b[39m, in \u001b[36mDirectorAgent.train_goal_vae_step\u001b[39m\u001b[34m(self, imagine_rollout)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# KL divergence\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# [B, L] -> [B]\u001b[39;00m\n\u001b[32m    673\u001b[39m kl_loss = torch.distributions.kl_divergence(\n\u001b[32m    674\u001b[39m     encoded_dist, \u001b[38;5;28mself\u001b[39m.skill_prior\n\u001b[32m    675\u001b[39m ).mean((-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m kl_coef = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkl_controller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkl_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    678\u001b[39m vae_loss = (recon_loss + kl_coef * kl_loss).mean()  \u001b[38;5;66;03m# [B] -> scalar\u001b[39;00m\n\u001b[32m    680\u001b[39m \u001b[38;5;66;03m# --- Backward pass for VAE only ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_RL/lib/python3.13/site-packages/torchrl/data/llm/utils.py:122\u001b[39m, in \u001b[36mAdaptiveKLController.update\u001b[39m\u001b[34m(self, kl_values)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# renormalize kls\u001b[39;00m\n\u001b[32m    121\u001b[39m kl_value = -torch.as_tensor(kl_values).mean() / \u001b[38;5;28mself\u001b[39m.coef\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m proportional_error = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkl_value\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ϵₜ\u001b[39;00m\n\u001b[32m    123\u001b[39m mult = \u001b[32m1\u001b[39m + proportional_error * n_steps / \u001b[38;5;28mself\u001b[39m.horizon\n\u001b[32m    124\u001b[39m \u001b[38;5;28mself\u001b[39m.coef *= mult  \u001b[38;5;66;03m# βₜ₊₁\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_RL/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2330\u001b[39m, in \u001b[36mclip\u001b[39m\u001b[34m(a, a_min, a_max, out, min, max, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_RL/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:66\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(*args, **kwds)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_RL/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:42\u001b[39m, in \u001b[36m_wrapit\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapit\u001b[39m(obj, method, *args, **kwds):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     conv = \u001b[43m_array_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001b[39;00m\n\u001b[32m     45\u001b[39m     arr, = conv.as_arrays(subok=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_RL/lib/python3.13/site-packages/torch/_tensor.py:1194\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Update agent with imagined data\n",
    "metrics = agent.update(imagined_rollout)\n",
    "# <<< Train agent part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goal_VAE_loss': -685.745361328125,\n",
      " 'goal_kl_loss': 0.2932986617088318,\n",
      " 'goal_recon_loss': -685.745361328125,\n",
      " 'manager_ActorCritic/S': 0.040681224316358566,\n",
      " 'manager_ActorCritic/critic_loss': 12.936750411987305,\n",
      " 'manager_ActorCritic/entropy_loss': 1.984375,\n",
      " 'manager_ActorCritic/norm_ratio': 1.0,\n",
      " 'manager_ActorCritic/policy_loss': 1.179916501045227,\n",
      " 'manager_ActorCritic/total_loss': 12.233854293823242,\n",
      " 'success_manager': 0.0,\n",
      " 'worker_ActorCritic/S': 0.021855171769857407,\n",
      " 'worker_ActorCritic/critic_loss': 11.247394561767578,\n",
      " 'worker_ActorCritic/entropy_loss': 2.140625,\n",
      " 'worker_ActorCritic/norm_ratio': 1.0,\n",
      " 'worker_ActorCritic/policy_loss': -0.05270551145076752,\n",
      " 'worker_ActorCritic/total_loss': 9.16343879699707}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the metrics\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final full call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriju11-mukherjee\u001b[0m (\u001b[33mrm_ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/I6347325/work_space/STORM/wandb/run-20250406_155037-5a8u907f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rm_ai/WMBRL/runs/5a8u907f' target=\"_blank\">TEM-Transformer_1</a></strong> to <a href='https://wandb.ai/rm_ai/WMBRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rm_ai/WMBRL' target=\"_blank\">https://wandb.ai/rm_ai/WMBRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rm_ai/WMBRL/runs/5a8u907f' target=\"_blank\">https://wandb.ai/rm_ai/WMBRL/runs/5a8u907f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env: \u001b[33mALE/MsPacman-v5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSaving model at total steps 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 931/15000 [00:01<00:16, 870.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_imagine_buffer: 1024x16@torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1155/15000 [01:39<2:22:26,  1.62it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "# with wandb.init(\n",
    "#     project=\"WMBRL\",  # Replace with your project name\n",
    "#     name=run_params.exp_name,   # Use the experiment name from RunParam\n",
    "#     config = {\n",
    "#         \"env_name\": run_params.env_name,\n",
    "#         \"seed\": run_params.seed,\n",
    "#     }\n",
    "# ) as run:\n",
    "    # Log the configuration to wandb\n",
    "    # run.config.update(run_params.conf)\n",
    "    # run.log({\"WM_params\": f\"{world_model_params:.2e}\", \"Agent_params\": f\"{agent_params:.2e}\"})\n",
    "    # logger = WandbLogger(run)\n",
    "    # train\n",
    "joint_train_world_model_agent(\n",
    "    env_name=run_params.env_name,\n",
    "    num_envs=run_params.conf.JointTrainAgent.NumEnvs,\n",
    "    max_steps=run_params.conf.JointTrainAgent.SampleMaxSteps,\n",
    "    image_size=run_params.conf.BasicSettings.ImageSize,\n",
    "    replay_buffer=replay_buffer,\n",
    "    world_model=world_model,\n",
    "    agent=agent,\n",
    "    train_dynamics_every_steps=run_params.conf.JointTrainAgent.TrainDynamicsEverySteps,\n",
    "    train_agent_every_steps=run_params.conf.JointTrainAgent.TrainAgentEverySteps,\n",
    "    batch_size=run_params.conf.JointTrainAgent.BatchSize,\n",
    "    demonstration_batch_size=(\n",
    "        run_params.conf.JointTrainAgent.DemonstrationBatchSize\n",
    "        if run_params.conf.JointTrainAgent.UseDemonstration\n",
    "        else 0\n",
    "    ),\n",
    "    batch_length=run_params.conf.JointTrainAgent.BatchLength,\n",
    "    imagine_batch_size=run_params.conf.JointTrainAgent.ImagineBatchSize,\n",
    "    imagine_demonstration_batch_size=(\n",
    "        run_params.conf.JointTrainAgent.ImagineDemonstrationBatchSize\n",
    "        if run_params.conf.JointTrainAgent.UseDemonstration\n",
    "        else 0\n",
    "    ),\n",
    "    imagine_context_length=run_params.conf.JointTrainAgent.ImagineContextLength,\n",
    "    imagine_batch_length=run_params.conf.JointTrainAgent.ImagineBatchLength,\n",
    "    save_every_steps=run_params.conf.JointTrainAgent.SaveEverySteps,\n",
    "    seed=run_params.seed,\n",
    "    logger=logger,\n",
    "    args=run_params,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
