{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import ale_py\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import colorama\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import os\n",
    "# import wandb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/Users/rijulizer/work_space/Thesis/STORM\"))# Dynamically reload the modules to reflect any changes\n",
    "\n",
    "import utils\n",
    "import sub_models.replay_buffer\n",
    "import env_wrapper\n",
    "# import agents\n",
    "import sub_models.director_agents\n",
    "import sub_models.functions_losses\n",
    "import sub_models.world_models\n",
    "import sub_models.constants\n",
    "import train\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(sub_models.replay_buffer)\n",
    "importlib.reload(env_wrapper)\n",
    "importlib.reload(sub_models.director_agents)\n",
    "importlib.reload(sub_models.functions_losses)\n",
    "importlib.reload(sub_models.world_models)\n",
    "importlib.reload(sub_models.constants)\n",
    "importlib.reload(train)\n",
    "\n",
    "from utils import seed_np_torch, Logger, load_config\n",
    "from sub_models.replay_buffer import ReplayBuffer\n",
    "from train import (\n",
    "    build_single_env,\n",
    "    build_vec_env,\n",
    "    build_world_model,\n",
    "    build_agent,\n",
    "    train_world_model_step,\n",
    "    world_model_imagine_data,\n",
    "    joint_train_world_model_agent,\n",
    ")\n",
    "from sub_models.constants import DEVICE\n",
    "print(DEVICE, DEVICE.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mArguments:\u001b[0m\n",
      "\u001b[32m-----------------\u001b[0m\n",
      "\u001b[32mexp_name: \u001b[0mTEM-Transformer_1\n",
      "\u001b[32mseed: \u001b[0m1\n",
      "\u001b[32mconfig_path: \u001b[0m../config_files/STORM.yaml\n",
      "\u001b[32mtrajectory_path: \u001b[0mD_TRAJ/MsPacman.pkl\n",
      "\u001b[32menv_name: \u001b[0mALE/MsPacman-v5\n",
      "\u001b[32m-----------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "class RunParams:\n",
    "    def __init__(self, env_name=\"MsPacman\", exp_name = \"TEM-Transformer\"):\n",
    "        self._env_name = env_name\n",
    "        self.exp_name = exp_name\n",
    "        self.seed = 1\n",
    "        self.config_path = \"../config_files/STORM.yaml\"\n",
    "        self.trajectory_path = f\"D_TRAJ/{self._env_name}.pkl\"\n",
    "        self.env_name = f\"ALE/{self._env_name}-v5\"\n",
    "\n",
    "        self.conf = load_config(self.config_path)\n",
    "        self.print_args()\n",
    "    def print_args(self):\n",
    "        print(colorama.Fore.GREEN + \"Arguments:\" + colorama.Style.RESET_ALL)\n",
    "        print(colorama.Fore.GREEN + \"-----------------\" + colorama.Style.RESET_ALL)\n",
    "        print(colorama.Fore.GREEN + \"exp_name: \" + colorama.Style.RESET_ALL + self.exp_name)\n",
    "        print(colorama.Fore.GREEN + \"seed: \" + colorama.Style.RESET_ALL + str(self.seed))\n",
    "        print(colorama.Fore.GREEN + \"config_path: \" + colorama.Style.RESET_ALL + self.config_path)\n",
    "        print(colorama.Fore.GREEN + \"trajectory_path: \" + colorama.Style.RESET_ALL + self.trajectory_path)\n",
    "        print(colorama.Fore.GREEN + \"env_name: \" + colorama.Style.RESET_ALL + self.env_name)\n",
    "        print(colorama.Fore.GREEN + \"-----------------\" + colorama.Style.RESET_ALL)\n",
    "    \n",
    "    # def get_configs(self):\n",
    "        \n",
    "    #     config_dict = {\n",
    "    #         \"env_ImageSize\": self.conf[\"BasicSettings\"][\"ImageSize\"],\n",
    "    #         \"env_ReplayBufferOnGPU\": self.conf[\"BasicSettings\"][\"ReplayBufferOnGPU\"],\n",
    "    #         \"WM_InChannels\": self.conf[\"Models\"][\"WorldModel\"][\"InChannels\"],\n",
    "    #         \"WM_TransformerMaxLength\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerMaxLength\"],\n",
    "    #         \"WM_TransformerHiddenDim\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerHiddenDim\"],\n",
    "    #         \"WM_TransformerNumLayers\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerNumLayers\"],\n",
    "    #         \"WM_TransformerNumHeads\": self.conf[\"Models\"][\"WorldModel\"][\"TransformerNumHeads\"],\n",
    "    #         \"Agent_NumLayers\": self.conf[\"Models\"][\"Agent\"][\"NumLayers\"],\n",
    "    #         \"Agent_HiddenDim\": self.conf[\"Models\"][\"Agent\"][\"HiddenDim\"],\n",
    "    #         \"Agent_Gamma\": self.conf[\"Models\"][\"Agent\"][\"Gamma\"],\n",
    "    #         \"Agent_Lambda\": self.conf[\"Models\"][\"Agent\"][\"Lambda\"],\n",
    "    #         \"Agent_EntropyCoef\": self.conf[\"Models\"][\"Agent\"][\"EntropyCoef\"],\n",
    "    #         \"Train_MaxSteps\": self.conf[\"JointTrainAgent\"][\"SampleMaxSteps\"],\n",
    "    #         \"Train_BufferMaxLength\": self.conf[\"JointTrainAgent\"][\"BufferMaxLength\"],\n",
    "    #         \"Train_BufferWarmUp\": self.conf[\"JointTrainAgent\"][\"BufferWarmUp\"],\n",
    "    #         \"Train_NumEnvs\": self.conf[\"JointTrainAgent\"][\"NumEnvs\"],\n",
    "    #         \"Train_BatchSize\": self.conf[\"JointTrainAgent\"][\"BatchSize\"],\n",
    "    #         \"Train_DemonstrationBatchSize\": self.conf[\"JointTrainAgent\"][\"DemonstrationBatchSize\"],\n",
    "    #         \"Train_BatchLength\": self.conf[\"JointTrainAgent\"][\"BatchLength\"],\n",
    "    #         \"Train_ImagineBatchSize\": self.conf[\"JointTrainAgent\"][\"ImagineBatchSize\"],\n",
    "    #         \"Train_ImagineDemonstrationBatchSize\": self.conf[\"JointTrainAgent\"][\"ImagineDemonstrationBatchSize\"],\n",
    "    #         \"Train_ImagineContextLength\": self.conf[\"JointTrainAgent\"][\"ImagineContextLength\"],\n",
    "    #         \"Train_ImagineBatchLength\": self.conf[\"JointTrainAgent\"][\"ImagineBatchLength\"],\n",
    "    #         \"Train_TrainDynamicsEverySteps\": self.conf[\"JointTrainAgent\"][\"TrainDynamicsEverySteps\"],\n",
    "    #         \"Train_TrainAgentEverySteps\": self.conf[\"JointTrainAgent\"][\"TrainAgentEverySteps\"],\n",
    "    #         \"Train_SaveEverySteps\": self.conf[\"JointTrainAgent\"][\"SaveEverySteps\"],\n",
    "    #         \"Train_UseDemonstration\": self.conf[\"JointTrainAgent\"][\"UseDemonstration\"],\n",
    "    #     }\n",
    "    #     return config_dict\n",
    "\n",
    "run_params = RunParams(env_name=\"MsPacman\", exp_name = \"TEM-Transformer_1\")\n",
    "# set seed\n",
    "seed_np_torch(seed=run_params.seed)\n",
    "# tensorboard writer\n",
    "logger = Logger(path=f\"runs/{run_params.exp_name}\")\n",
    "# copy config file\n",
    "# shutil.copy(run_params.config_path, f\"runs/{run_params.exp_name}/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World model transformer: StochasticTransformerKVCache\n",
      "World model parameters: 16508547\n",
      "Agent parameters: 5367561\n"
     ]
    }
   ],
   "source": [
    "# Setuop env, models, replay buffer\n",
    "# getting action_dim with dummy env\n",
    "dummy_env = build_single_env(\n",
    "    run_params.env_name, run_params.conf.BasicSettings.ImageSize, seed=0\n",
    ")\n",
    "action_dim = dummy_env.action_space.n\n",
    "\n",
    "# build world model and agent\n",
    "world_model = build_world_model(run_params.conf, action_dim)\n",
    "agent = build_agent(run_params.conf, action_dim)\n",
    "print(f\"World model transformer: {world_model.storm_transformer.__class__.__name__}\")\n",
    "# Log the number of parameters for both models\n",
    "world_model_params = sum(p.numel() for p in world_model.parameters() if p.requires_grad)\n",
    "agent_params = sum(p.numel() for p in agent.parameters() if p.requires_grad)\n",
    "print(f\"World model parameters: {world_model_params}\")\n",
    "print(f\"Agent parameters: {agent_params}\")\n",
    "# Build replay buffer\n",
    "replay_buffer = ReplayBuffer(\n",
    "    num_envs=run_params.conf.JointTrainAgent.NumEnvs,\n",
    "    obs_shape=(run_params.conf.BasicSettings.ImageSize, run_params.conf.BasicSettings.ImageSize, 3),\n",
    "    max_length=run_params.conf.JointTrainAgent.BufferMaxLength,\n",
    "    warmup_length=20,  #FIXME: run_params.conf.JointTrainAgent.BufferWarmUp,\n",
    "    store_on_gpu=run_params.conf.BasicSettings.ReplayBufferOnGPU,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 16, 2500, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_params.conf.JointTrainAgent.ImagineContextLength, run_params.conf.JointTrainAgent.ImagineBatchLength, run_params.conf.JointTrainAgent.SaveEverySteps,run_params.seed,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env: \u001b[33mALE/MsPacman-v5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSaving model at total steps 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = joint_train_world_model_agent(\n",
    "    env_name=run_params.env_name,\n",
    "    num_envs=run_params.conf.JointTrainAgent.NumEnvs,\n",
    "    max_steps=64,\n",
    "    image_size=run_params.conf.BasicSettings.ImageSize,\n",
    "    replay_buffer=replay_buffer,\n",
    "    world_model=world_model,\n",
    "    agent=agent,\n",
    "    train_dynamics_every_steps=1,\n",
    "    train_agent_every_steps=1,\n",
    "    batch_size=64,\n",
    "    demonstration_batch_size=0,\n",
    "    batch_length=16,\n",
    "    imagine_batch_size=64,\n",
    "    imagine_demonstration_batch_size=0,\n",
    "    imagine_context_length=8,\n",
    "    imagine_batch_length=16,\n",
    "    save_every_steps=2500,\n",
    "    seed=run_params.seed,\n",
    "    logger=logger,\n",
    "    args=run_params,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown: joint_train_world_model_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env: \u001b[33mALE/MsPacman-v5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "## setup variable names for breakdown\n",
    "env_name=run_params.env_name\n",
    "num_envs=run_params.conf.JointTrainAgent.NumEnvs\n",
    "max_steps=run_params.conf.JointTrainAgent.SampleMaxSteps\n",
    "image_size=run_params.conf.BasicSettings.ImageSize\n",
    "train_dynamics_every_steps=run_params.conf.JointTrainAgent.TrainDynamicsEverySteps\n",
    "train_agent_every_steps=run_params.conf.JointTrainAgent.TrainAgentEverySteps\n",
    "batch_size=3 #FIXME: run_params.conf.JointTrainAgent.BatchSize\n",
    "demonstration_batch_size=(\n",
    "    run_params.conf.JointTrainAgent.DemonstrationBatchSize\n",
    "    if run_params.conf.JointTrainAgent.UseDemonstration\n",
    "    else 0\n",
    ")\n",
    "batch_length=16 #FIXME: run_params.conf.JointTrainAgent.BatchLength\n",
    "imagine_batch_size=run_params.conf.JointTrainAgent.ImagineBatchSize\n",
    "imagine_demonstration_batch_size=(\n",
    "    run_params.conf.JointTrainAgent.ImagineDemonstrationBatchSize\n",
    "    if run_params.conf.JointTrainAgent.UseDemonstration\n",
    "    else 0\n",
    ")\n",
    "imagine_context_length=run_params.conf.JointTrainAgent.ImagineContextLength\n",
    "imagine_batch_length=16 #FIXME: run_params.conf.JointTrainAgent.ImagineBatchLength\n",
    "save_every_steps=run_params.conf.JointTrainAgent.SaveEverySteps\n",
    "seed=run_params.seed\n",
    "args=run_params\n",
    "\n",
    "\n",
    "## Setup env\n",
    "vec_env = build_vec_env(env_name, image_size, num_envs=1, seed=seed)\n",
    "print(\n",
    "    \"Current env: \"\n",
    "    + colorama.Fore.YELLOW\n",
    "    + f\"{env_name}\"\n",
    "    + colorama.Style.RESET_ALL\n",
    ")\n",
    "\n",
    "# reset envs and variables\n",
    "sum_reward = np.zeros(num_envs)\n",
    "current_obs, current_info = vec_env.reset()\n",
    "context_obs = deque(maxlen=16)\n",
    "context_action = deque(maxlen=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from env part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 173.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer ready 21\n",
      "Replay buffer ready 22\n",
      "Replay buffer ready 23\n",
      "Replay buffer ready 24\n",
      "Replay buffer ready 25\n",
      "Replay buffer ready 26\n",
      "Replay buffer ready 27\n",
      "Replay buffer ready 28\n",
      "Replay buffer ready 29\n",
      "Replay buffer ready 30\n",
      "Replay buffer ready 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for total_steps in tqdm(range(32)):\n",
    "    # sample part >>>\n",
    "    if replay_buffer.ready:  # ready only after warmpup\n",
    "        print(\"Replay buffer ready\", total_steps)\n",
    "        # WM and Agent are in eval mode\n",
    "        world_model.eval()\n",
    "        agent.eval()\n",
    "        with torch.no_grad():\n",
    "            if len(context_action) == 0:\n",
    "                # this is the case in the first step\n",
    "                action = vec_env.action_space.sample()\n",
    "            else:\n",
    "                context_latent = world_model.encode_obs(\n",
    "                    torch.cat(list(context_obs), dim=1)\n",
    "                )\n",
    "                model_context_action = np.stack(list(context_action), axis=1)\n",
    "                model_context_action = torch.Tensor(model_context_action).to(DEVICE)\n",
    "                prior_flattened_sample, last_dist_feat = (\n",
    "                    world_model.calc_last_dist_feat(\n",
    "                        context_latent, model_context_action\n",
    "                    )\n",
    "                )\n",
    "                latent = torch.cat([prior_flattened_sample, last_dist_feat], dim=-1)\n",
    "                # get the action, goal and skill from the agent\n",
    "                action = agent.sample_as_env_action(latent)\n",
    "        # [B, H, W, C] -> [B, 1, C, H, W] # B=1\n",
    "        context_obs.append(\n",
    "            torch.permute(\n",
    "                torch.tensor(current_obs, device=DEVICE), (0, 3, 1, 2)\n",
    "            ).unsqueeze(1)\n",
    "            / 255\n",
    "        )\n",
    "        context_action.append(action)\n",
    "    else:\n",
    "        # simply sample random action\n",
    "        action = vec_env.action_space.sample()\n",
    "\n",
    "    # Perform action in the env and observe the next state, reward, done, truncated\n",
    "    obs, reward, done, truncated, info = vec_env.step(action)\n",
    "\n",
    "    # Append the transition to the replay buffer\n",
    "    replay_buffer.append(\n",
    "        current_obs, action, reward, np.logical_or(done, info[\"life_loss\"])\n",
    "        )\n",
    "\n",
    "    done_flag = np.logical_or(done, truncated)\n",
    "    if done_flag.any():  # end of episode\n",
    "        for i in range(num_envs):\n",
    "            if done_flag[i]:\n",
    "                sum_reward[i] = 0\n",
    "\n",
    "    # Update current_obs, current_info and sum_reward\n",
    "    sum_reward += reward\n",
    "    current_obs = obs\n",
    "    current_info = info\n",
    "    # <<< sample part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 64, 64, 3), (1,), (1,), (1,), (1,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape, action.shape, reward.shape, done.shape, truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually fill the replay buffer goal and skill\n",
    "# goal and skill should ideally be appended in the replay buffer\n",
    "# replay_buffer.buffer[\"goal\"][0:32] = torch.zeros(32,1,1024)\n",
    "# replay_buffer.buffer[\"skill\"][0:32] = torch.zeros(32,1, 8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train world model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train world model part >>>\n",
    "train_world_model_step(\n",
    "    replay_buffer=replay_buffer,\n",
    "    world_model=world_model,\n",
    "    batch_size=batch_size,\n",
    "    demonstration_batch_size=demonstration_batch_size,\n",
    "    batch_length=batch_length,\n",
    "    logger=logger,\n",
    ")\n",
    "##<<< Train world model part\n",
    "## Breakdown of the above code\n",
    "# Sample from replay buffer\n",
    "# buffer_sample = replay_buffer.sample(\n",
    "#     batch_size, demonstration_batch_size, batch_length\n",
    "# )\n",
    "# for key, value in buffer_sample.items():\n",
    "#     print(f\"{key}, Value shape: {value.shape}\")\n",
    "# obs, Value shape: torch.Size([3, 16, 3, 64, 64])\n",
    "# action, Value shape: torch.Size([3, 16])\n",
    "# reward, Value shape: torch.Size([3, 16])\n",
    "# termination, Value shape: torch.Size([3, 16])\n",
    "# goal, Value shape: torch.Size([3, 16])\n",
    "# skill, Value shape: torch.Size([3, 16])\n",
    "# print(f\"Shapes of obs: {obs.shape}, action: {action.shape}, reward: {reward.shape}, termination: {termination.shape}\")\n",
    "\n",
    "## Train world model with the sampled data\n",
    "# world_model.update(buffer_sample[\"obs\"], buffer_sample[\"action\"], buffer_sample[\"reward\"], buffer_sample[\"termination\"], logger=logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_imagine_buffer: 1024x16@torch.float32\n",
      "Shape of sample: torch.Size([1024, 17, 1024])\n",
      "Shape of hidden: torch.Size([1024, 17, 512])\n",
      "Shape of action: torch.Size([1024, 16])\n",
      "Shape of reward: torch.Size([1024, 16])\n",
      "Shape of termination: torch.Size([1024, 16])\n",
      "Shape of goal: torch.Size([1024, 16, 1024])\n",
      "Shape of skill: torch.Size([1024, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Train agent part >>>\n",
    "# print(\"Training Agent...\")\n",
    "log_video = False\n",
    "imagined_rollout = world_model_imagine_data(\n",
    "    replay_buffer=replay_buffer,\n",
    "    world_model=world_model,\n",
    "    agent=agent,\n",
    "    imagine_batch_size=imagine_batch_size,\n",
    "    imagine_demonstration_batch_size=imagine_demonstration_batch_size,\n",
    "    imagine_context_length=imagine_context_length,\n",
    "    imagine_batch_length=imagine_batch_length,\n",
    "    log_video=log_video,\n",
    "    logger=logger,\n",
    ")\n",
    "for k, v  in imagined_rollout.items():\n",
    "    print(f\"Shape of {k}: {v.shape}\")\n",
    "\n",
    "## breakdown : world_model_imagine_data\n",
    "# imagine_batch_size = 3\n",
    "# world_model.eval()\n",
    "# agent.eval()\n",
    "\n",
    "# buffer_sample = replay_buffer.sample(\n",
    "#     imagine_batch_size, imagine_demonstration_batch_size, imagine_context_length\n",
    "# )\n",
    "# print(f\"Buffer sample items:\")\n",
    "# for k, v  in buffer_sample.items():\n",
    "#     print(f\"Shape of {k}: {v.shape}\")\n",
    "\n",
    "# imagined_rollout = world_model.imagine_data(\n",
    "#     agent,\n",
    "#     buffer_sample,\n",
    "#     imagine_batch_size=imagine_batch_size + imagine_demonstration_batch_size,\n",
    "#     imagine_batch_length=imagine_batch_length,\n",
    "#     log_video=log_video,\n",
    "#     logger=logger,\n",
    "# )\n",
    "# print(f\"\\n\\nImagine rollout items:\")\n",
    "# for k, v  in imagined_rollout.items():\n",
    "#     print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Imagine rollout items:\n",
      "sample: torch.Size([1024, 16, 1024])\n",
      "hidden: torch.Size([1024, 16, 512])\n",
      "action: torch.Size([1024, 16])\n",
      "reward: torch.Size([1024, 16])\n",
      "termination: torch.Size([1024, 16])\n",
      "goal: torch.Size([1024, 16, 1024])\n",
      "skill: torch.Size([1024, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# For the sake of testing the code, L=16\n",
    "imagined_rollout[\"sample\"] = imagined_rollout[\"sample\"][:, 0:16]\n",
    "imagined_rollout[\"hidden\"] = imagined_rollout[\"hidden\"][:, 0:16]\n",
    "print(f\"\\n\\nImagine rollout items:\")\n",
    "for k, v  in imagined_rollout.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update agent with imagined data\n",
    "metrics = agent.update(imagined_rollout)\n",
    "# <<< Train agent part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goal_VAE_loss': -685.745361328125,\n",
      " 'goal_kl_loss': 0.2932986617088318,\n",
      " 'goal_recon_loss': -685.745361328125,\n",
      " 'manager_ActorCritic/S': 0.040681224316358566,\n",
      " 'manager_ActorCritic/critic_loss': 12.936750411987305,\n",
      " 'manager_ActorCritic/entropy_loss': 1.984375,\n",
      " 'manager_ActorCritic/norm_ratio': 1.0,\n",
      " 'manager_ActorCritic/policy_loss': 1.179916501045227,\n",
      " 'manager_ActorCritic/total_loss': 12.233854293823242,\n",
      " 'success_manager': 0.0,\n",
      " 'worker_ActorCritic/S': 0.021855171769857407,\n",
      " 'worker_ActorCritic/critic_loss': 11.247394561767578,\n",
      " 'worker_ActorCritic/entropy_loss': 2.140625,\n",
      " 'worker_ActorCritic/norm_ratio': 1.0,\n",
      " 'worker_ActorCritic/policy_loss': -0.05270551145076752,\n",
      " 'worker_ActorCritic/total_loss': 9.16343879699707}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the metrics\n",
    "pprint(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
